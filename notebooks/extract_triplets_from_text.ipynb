{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations, product\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "\n",
    "import torch\n",
    "\n",
    "string.punctuation += '’'\n",
    "string.punctuation += '–'\n",
    "string.punctuation += '”'\n",
    "\n",
    "\n",
    "def get_predictions(sentence, attentions_types, use_bert, use_lmms, lr_bin, lr_multi, tokenizer, encoder, nlp, threshold_bin=0.5):\n",
    "    pred_list = []\n",
    "    emb_sent = get_embeddings(sentence, attentions_types, use_bert, use_lmms, tokenizer, encoder, nlp)\n",
    "    for emb in tqdm(emb_sent, total=len(emb_sent), leave=False):\n",
    "        binary_conf = lr_bin.predict_proba(emb[0].reshape(1, -1))[0][1]\n",
    "\n",
    "        if binary_conf > threshold_bin:\n",
    "            predicted_label = list(lr_multi.predict(emb[0].reshape(1, -1)))[0]\n",
    "            triplet = emb[1]\n",
    "            pred_list.append((predicted_label, triplet, binary_conf))\n",
    "    return pred_list\n",
    "\n",
    "def process(sentence, tokenizer, nlp, return_pt=True):\n",
    "    doc = nlp(sentence)\n",
    "    tokens = list(doc)\n",
    "\n",
    "    chunk2id = {}\n",
    "\n",
    "    start_chunk = []\n",
    "    end_chunk = []\n",
    "    noun_chunks = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        noun_chunks.append(chunk.text)\n",
    "        start_chunk.append(chunk.start)\n",
    "        end_chunk.append(chunk.end)\n",
    "\n",
    "    sentence_mapping = []\n",
    "    token2id = {}\n",
    "    mode = 0 # 1 in chunk, 0 not in chunk       \n",
    "    chunk_id = 0\n",
    "    for idx, token in enumerate(doc):\n",
    "        if idx in start_chunk:\n",
    "            mode = 1\n",
    "            sentence_mapping.append(noun_chunks[chunk_id])\n",
    "            if sentence_mapping[-1] not in token2id:\n",
    "                token2id[sentence_mapping[-1]] = len(token2id)\n",
    "            chunk_id += 1\n",
    "        elif idx in end_chunk:\n",
    "            mode = 0\n",
    "\n",
    "        if mode == 0:\n",
    "            sentence_mapping.append(token.text)\n",
    "            if sentence_mapping[-1] not in token2id:\n",
    "                token2id[sentence_mapping[-1]] = len(token2id)\n",
    "\n",
    "\n",
    "    token_ids = []\n",
    "    tokenid2word_mapping = []\n",
    "\n",
    "    for token in sentence_mapping:\n",
    "        subtoken_ids = tokenizer(str(token), add_special_tokens=False)['input_ids']\n",
    "        tokenid2word_mapping += [ token2id[token] ]*len(subtoken_ids)\n",
    "        token_ids += subtoken_ids\n",
    "\n",
    "    tokenizer_name = str(tokenizer.__str__)\n",
    "\n",
    "    outputs = {\n",
    "        'input_ids': [tokenizer.cls_token_id] + token_ids + [tokenizer.sep_token_id],\n",
    "        'attention_mask': [1]*(len(token_ids)+2),\n",
    "        'token_type_ids': [0]*(len(token_ids)+2)\n",
    "    }\n",
    "\n",
    "    if return_pt:\n",
    "        for key, value in outputs.items():\n",
    "            outputs[key] = torch.from_numpy(np.array(value)).long().unsqueeze(0)\n",
    "    \n",
    "    return outputs, tokenid2word_mapping, token2id, noun_chunks, sentence_mapping\n",
    "\n",
    "\n",
    "def deduplication(pred_list):\n",
    "    pred_max_conf = {}\n",
    "    filtered_pred = {}\n",
    "\n",
    "    for ind, pred in enumerate(pred_list):\n",
    "        pred_triplet = (pred[1][0], pred[1][1])\n",
    "\n",
    "        if pred_triplet not in filtered_pred.keys():\n",
    "            pred_max_conf[pred_triplet] = pred[2]\n",
    "            filtered_pred[pred_triplet] = pred\n",
    "\n",
    "        elif pred_triplet in filtered_pred and pred[2] > pred_max_conf[pred_triplet]:\n",
    "            pred_max_conf[pred_triplet] = pred[2]\n",
    "            filtered_pred[pred_triplet] = pred\n",
    "    \n",
    "    sorted_pred = sorted(list(filtered_pred.values()), key=lambda x: x[2], reverse=True)\n",
    "    prediction = [el[1] for el in sorted_pred]\n",
    "    predicted_labels = [el[0] for el in sorted_pred]\n",
    "    \n",
    "    return prediction, predicted_labels\n",
    "\n",
    "\n",
    "def compare_triplets(targets, predict, dist_thresh=0.2):\n",
    "    compare_result = []\n",
    "    for target in targets:\n",
    "        sub_compare = []\n",
    "        for target, predict_ in zip(target, predict):\n",
    "            answer =  False\n",
    "            dist = jaccard_distance(set(target.lower()), set(predict_.lower()))\n",
    "            if predict_ in target or dist < dist_thresh or target in predict_:\n",
    "                answer = True\n",
    "            sub_compare.append(answer)\n",
    "        sub_compare = all(sub_compare)\n",
    "        compare_result.append(sub_compare)\n",
    "    return any(compare_result)\n",
    "\n",
    "\n",
    "def compute_metrics(df, lr_bin, lr_multi, best_attentions, tokenizer, encoder, nlp):\n",
    "    fp, tp, fn, rel_pred_count = 0, 0, 0, 0\n",
    "    tp_predicts_dict, fp_predicts_dict = OrderedDict({}), OrderedDict({})\n",
    "    preds_dict = OrderedDict({})\n",
    "    prs, recs, f1s = [], [], []\n",
    "    \n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0]):\n",
    "        \n",
    "        tp_local, fp_local, fn_local = 0, 0, 0\n",
    "        collected_predictions = []\n",
    "        cutted_text = sent_tokenize(row.text)\n",
    "        for cut_text in cutted_text:\n",
    "            try:\n",
    "                predictions = get_predictions(cut_text, best_attentions, False, False, lr_bin, lr_multi, tokenizer=tokenizer, encoder=encoder, nlp=nlp, threshold_bin=0.7)\n",
    "            except:\n",
    "                predictions = []\n",
    "            filtered_predictions, _ = deduplication(predictions)\n",
    "            if len(filtered_predictions):\n",
    "                collected_predictions.extend(filtered_predictions)\n",
    "                \n",
    "        preds_dict[row.text] = list(set(collected_predictions))\n",
    "        targets = eval(row.target)\n",
    "        target_triplets = [target[:3] for target in targets]\n",
    "        tp_predicts = []\n",
    "        fp_predicts = []\n",
    "\n",
    "        for predict in collected_predictions:\n",
    "            score_bool = compare_triplets(target_triplets, predict)\n",
    "\n",
    "            if score_bool:\n",
    "                tp_predicts.append(predict)\n",
    "                tp += 1\n",
    "                tp_local += 1\n",
    "            else:\n",
    "                fp_predicts.append(predict)\n",
    "                fp += 1\n",
    "                fp_local += 1\n",
    "\n",
    "        tp_predicts_dict[row.text] = tp_predicts\n",
    "\n",
    "        fp_predicts_dict[row.text] = fp_predicts\n",
    "\n",
    "        for target in target_triplets:\n",
    "            score_bool = compare_triplets(filtered_predictions, target)\n",
    "            if not score_bool:\n",
    "                fn += 1\n",
    "                fn_local += 1\n",
    "        try:        \n",
    "            precision_local = tp_local / (tp_local + fp_local)\n",
    "            recall_local = tp_local / (tp_local + fn_local)\n",
    "            f1_local = 2 * (precision_local * recall_local) / (precision_local + recall_local)\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "        \n",
    "        prs.append(precision_local)\n",
    "        recs.append(recall_local)\n",
    "        f1s.append(f1_local)\n",
    "\n",
    "    assert len(prs) == len(recs) == len(f1s) == len(tp_predicts_dict) == len(fp_predicts_dict) == len(preds_dict)\n",
    "    \n",
    "    df['precision'] = prs\n",
    "    df['recall'] = recs\n",
    "    df['f1'] = f1s\n",
    "    df['tps'] = list(tp_predicts_dict.values())\n",
    "    df['fps'] = list(fp_predicts_dict.values())\n",
    "    df['preds'] = list(preds_dict.values())\n",
    "    \n",
    "    df.to_csv('../data/meta/trex_data_long_parsed.csv', index=False)\n",
    "    \n",
    "    try:        \n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        return precision, recall, f1\n",
    "    \n",
    "    except ZeroDivisionError:\n",
    "        return 0, 0, 0\n",
    "\n",
    "def get_vectorname(attentions_types, use_bert, use_lmms):\n",
    "    attentions_to_be_used = ['h-r', 'r-t', 'h-t', 'r-h', 't-r', 't-h'] \n",
    "    attentions_to_use = tuple([att for i, att in enumerate(attentions_to_be_used) if attentions_types[i] == 1])\n",
    "    name = '_'.join(attentions_to_use)\n",
    "    \n",
    "    if use_bert:\n",
    "        name += '_bert'\n",
    "    \n",
    "    if use_lmms:\n",
    "        name += '_lmms'\n",
    "        \n",
    "    return name\n",
    "\n",
    "\n",
    "\n",
    "def load_lr_models(vector_names):\n",
    "    with open(f'./logreg_models/lr_multi_{vector_names}.pkl', 'rb') as file:\n",
    "        lr_multi = pickle.load(file)\n",
    "\n",
    "    with open(f'./logreg_models/lr_bin_{vector_names}.pkl', 'rb') as file:\n",
    "        lr_bin = pickle.load(file)\n",
    "    \n",
    "    return lr_bin, lr_multi\n",
    "\n",
    "\n",
    "def compress_attention(attention, tokenid2word_mapping, operator=np.mean):\n",
    "\n",
    "    new_index = []\n",
    "    \n",
    "    prev = -1\n",
    "    for idx, row in enumerate(attention):\n",
    "        token_id = tokenid2word_mapping[idx]\n",
    "        if token_id != prev:\n",
    "            new_index.append( [row])\n",
    "            prev = token_id\n",
    "        else:\n",
    "            new_index[-1].append(row)\n",
    "\n",
    "    new_matrix = []\n",
    "    for row in new_index:\n",
    "        new_matrix.append(operator(np.array(row), 0))\n",
    "\n",
    "    new_matrix = np.array(new_matrix)\n",
    "\n",
    "    attention = np.array(new_matrix).T\n",
    "\n",
    "    prev = -1\n",
    "    new_index=  []\n",
    "    for idx, row in enumerate(attention):\n",
    "        token_id = tokenid2word_mapping[idx]\n",
    "        if token_id != prev:\n",
    "            new_index.append( [row])\n",
    "            prev = token_id\n",
    "        else:\n",
    "            new_index[-1].append(row)\n",
    "\n",
    "    \n",
    "    new_matrix = []\n",
    "    for row in new_index:\n",
    "        new_matrix.append(operator(np.array(row), 0))\n",
    "    \n",
    "    new_matrix = np.array(new_matrix)\n",
    "    \n",
    "    return new_matrix.T\n",
    "\n",
    "def get_outputs(sentence, tokenizer, encoder, nlp, use_cuda=True):\n",
    "\n",
    "    tokenizer_name = str(tokenizer.__str__)\n",
    "    inputs, tokenid2word_mapping, token2id, tokens, sentence_mapping = process(sentence, nlp=nlp, tokenizer=tokenizer, return_pt=True)\n",
    "    id2token = {value: key for key, value in token2id.items()}\n",
    "    for key in inputs.keys():\n",
    "        inputs[key] = inputs[key].cuda()\n",
    "    outputs = encoder(**inputs, output_attentions=True)\n",
    "    \n",
    "    return outputs[2], tokenid2word_mapping, token2id, sentence_mapping\n",
    "\n",
    "\n",
    "def get_embeddings(sentence, attentions_types, use_bert, use_lmms, tokenizer, encoder, nlp):\n",
    "    rel_pos = ['NN', 'NNP', 'NNS', 'JJR', 'JJS', 'MD', 'POS', 'VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ']\n",
    "    head_tail_pos = ['NN', 'NNP', 'NNS', 'PRP']\n",
    "\n",
    "    use_cuda = True    \n",
    "    att, tokenid2word_mapping, token2id, sentence_mapping = get_outputs(sentence, tokenizer, encoder, nlp, use_cuda=use_cuda)\n",
    "    \n",
    "    new_matr = []\n",
    "    \n",
    "    for layer in att:\n",
    "        for head in layer.squeeze():\n",
    "            attn = head.cpu()\n",
    "            attention_matrix = attn.detach().numpy()\n",
    "            attention_matrix = attention_matrix[1:-1, 1:-1]\n",
    "            \n",
    "            merged_attention = compress_attention(attention_matrix, tokenid2word_mapping)\n",
    "            \n",
    "            new_matr.append(merged_attention)\n",
    "    \n",
    "    new_matr = np.stack(new_matr)\n",
    "    \n",
    "    words = [token for token in sentence_mapping if token not in string.punctuation]\n",
    "    \n",
    "    nn_words = [word for word in words if nltk.pos_tag([word])[0][1] in head_tail_pos]\n",
    "    other_words = [word for word in words if nltk.pos_tag([word])[0][1] in rel_pos]\n",
    "    \n",
    "    triplets = [triplet for triplet in list(product(nn_words, nn_words, other_words)) \n",
    "                if triplet[0] != triplet[1] and triplet[0] != triplet[2] and triplet[1] != triplet[2]]\n",
    "    \n",
    "    \n",
    "    sent_embeddings = []\n",
    "    \n",
    "    for triplet in triplets:\n",
    "        head_ind = sentence_mapping.index(triplet[0])\n",
    "        tail_ind = sentence_mapping.index(triplet[1])\n",
    "        rel_ind = sentence_mapping.index(triplet[2])   \n",
    "\n",
    "        head_rel_emb = new_matr[:, head_ind, rel_ind]\n",
    "        rel_tail_emb = new_matr[:, rel_ind, tail_ind]\n",
    "        head_tail_emb = new_matr[:, head_ind, tail_ind]\n",
    "        rel_head_emb = new_matr[:, rel_ind, head_ind]\n",
    "        tail_rel_emb = new_matr[:, tail_ind, rel_ind]\n",
    "        tail_head_emb = new_matr[:, tail_ind, head_ind]\n",
    "        \n",
    "        attentions_to_be_used = [head_rel_emb, rel_tail_emb, head_tail_emb, rel_head_emb, tail_rel_emb, tail_head_emb] \n",
    "        attentions_to_use = tuple([att for i, att in enumerate(attentions_to_be_used) if attentions_types[i] == 1])\n",
    "\n",
    "        triplet_emb = np.concatenate(attentions_to_use, axis=0).squeeze()\n",
    "        sentence = ' '.join(sentence_mapping)\n",
    "        sent_embeddings.append((triplet_emb, triplet))\n",
    "        \n",
    "    return sent_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/spacy/util.py:833: UserWarning: [W095] Model 'en_core_web_sm' (3.1.0) was trained with spaCy v3.1 and may not be 100% compatible with the current version (3.2.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import en_core_web_sm\n",
    "\n",
    "from utils_inference import (get_vectorname, load_lr_models, compute_metrics)\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/spacy/util.py:833: UserWarning: [W095] Model 'en_core_web_sm' (3.1.0) was trained with spaCy v3.1 and may not be 100% compatible with the current version (3.2.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=r\"Passing\", category=FutureWarning)\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder    = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "encoder    = encoder.to(device)\n",
    "encoder    = encoder.eval()\n",
    "tokenizer  = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "df = pd.read_csv('../data/meta/trex_data_long.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_attentions = [1, 1, 1, 1, 1, 0]\n",
    "vectorname = get_vectorname(best_attentions, False, False)\n",
    "lr_bin, lr_multi = load_lr_models(vectorname)\n",
    "\n",
    "prec, rec, f1 = compute_metrics(df, lr_bin, lr_multi, best_attentions, tokenizer, encoder, nlp)\n",
    "\n",
    "print(prec, rec, f1)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
